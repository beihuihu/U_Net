{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gps\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.mask\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "import rasterio.merge\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio import features\n",
    "import fiona                     # I/O vector data (shape, geojson, ...)\n",
    "import pyproj                    # Change coordinate reference system\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import box, Point\n",
    "import json\n",
    "\n",
    "import numpy as np               # numerical array manipulation\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw\n",
    "from core.visualize import display_images\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required configurations (including the input and output paths) are stored in a separate file (such as config/Preprocessing.py)\n",
    "# Please provide required info in the file before continuing with this notebook. \n",
    "# hbh: in this scene,a new config named Preprocessing_within is created to distinguish from the original\n",
    "from config import Preprocessing   \n",
    "# In case you are using a different folder name such as configLargeCluster, then you should import from the respective folder \n",
    "# Eg. from configLargeCluster import Preprocessing\n",
    "config = Preprocessing.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=r'G:\\5_lakemapping\\sample635_12\\output'\n",
    "# raw_image_base_dir=r'G:\\5_lakemapping\\sample_img'\n",
    "# # output_dir=r'/media/nkd/backup/5_lakemapping/sample600/output'\n",
    "# # raw_image_base_dir=r'/media/nkd/backup/5_lakemapping/sample_img'\n",
    "# raw_ndwi_image_prefix = 'ndwi_int8_'\n",
    "# raw_bands_image_prefix = 'bands_int16_'\n",
    "# raw_image_file_type = '.tif'\n",
    "# extracted_file_type = '.png'\n",
    "# extracted_ndwi_filename = 'ndwi'\n",
    "# extracted_bands_filename = ['blue','green','red','swir']\n",
    "# extracted_annotation_filename = 'annotation'\n",
    "# type_num=6\n",
    "# ndwi_band = [0]# If raster has multiple channels, then bands will be [0, 1, ...] otherwise simply [0]\n",
    "# four_bands = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hbh: check whether the output dir(must be present) of each type is empty\n",
    "for i in range(0,type_num):\n",
    "    path_to_write=os.path.join(output_dir,'output'+str(i))\n",
    "    assert os.path.exists(path_to_write)\n",
    "    if not len(os.listdir(path_to_write))==0:\n",
    "         print('Warning: path_to_write{} is not empty! The old files in the directory may not be overwritten!!'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read the training area „ÄÅ training polygons\n",
    "trainingArea = gps.read_file(os.path.join(config.training_base_dir, config.training_area_fn))\n",
    "trainingPolygon = gps.read_file(os.path.join(config.training_base_dir, config.training_polygon_fn))\n",
    "# trainingArea = gps.read_file(r\"/media/nkd/backup/5_lakemapping/sample600/sample/area\")\n",
    "# trainingPolygon = gps.read_file(r\"/media/nkd/backup/5_lakemapping/sample600/sample/polygon\")\n",
    "\n",
    "print(trainingPolygon.shape,trainingArea.shape)# area:id, geomerry;   polygon:id, geometry \n",
    "trainingPolygon\n",
    "trainingArea\n",
    "print(f'Read a total of {trainingPolygon.shape[0]} object polygons and {trainingArea.shape[0]} training areas.')\n",
    "print(f'Polygons will be assigned to training areas in the next steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the training areas and the training polygons have the same crs     \n",
    "if trainingArea.crs  != trainingPolygon.crs:\n",
    "    print('Training area CRS does not match training_polygon CRS')\n",
    "    targetCRS = trainingPolygon.crs #Areas are less in number so conversion should be faster\n",
    "    trainingArea = trainingArea.to_crs(targetCRS)\n",
    "print(trainingPolygon.crs)\n",
    "print(trainingArea.crs)\n",
    "assert trainingPolygon.crs == trainingArea.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As input we received two shapefile, first one contains the training areas/rectangles and other contains the polygon of lakes/objects in those training areas\n",
    "# The first task is to determine the parent training area for each polygon.\n",
    "\n",
    "def dividePolygonsInTrainingAreas(trainingPolygon, trainingArea):\n",
    "    '''Assign annotated ploygons in to the training areas.'''\n",
    "    # For efficiency, assigned polygons are removed from the list, we make a copy here. \n",
    "    cpTrainingPolygon = trainingPolygon.copy()\n",
    "    splitPolygons = {}\n",
    "    for i in tqdm(trainingArea.index):\n",
    "        spTemp = [] \n",
    "        allocated = []\n",
    "        print(\"area's index:\",i)\n",
    "        for j in cpTrainingPolygon.index:\n",
    "            if cpTrainingPolygon.loc[j]['geometry'].intersects(trainingArea.loc[i]['geometry']):\n",
    "                spTemp.append(cpTrainingPolygon.loc[j])\n",
    "                allocated.append(j)      \n",
    "        splitPolygons[i] = {'polygons':spTemp,'bounds':list(trainingArea.bounds.loc[i]),'id':trainingArea.loc[i]['id'] ,'type':trainingArea.loc[i]['type']}\n",
    "        cpTrainingPolygon = cpTrainingPolygon.drop(allocated)#assigned polygons are removed from the list\n",
    "    return splitPolygons\n",
    "\n",
    "# areasWithPolygons contains the object polygons for each area!\n",
    "areasWithPolygons = dividePolygonsInTrainingAreas(trainingPolygon, trainingArea)\n",
    "print(f'Assigned training polygons in {len(areasWithPolygons)} training areas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(areasWithPolygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAreasThatOverlapWithTrainingData(areaInfo, writePath):\n",
    "    \"\"\"Iterates over raw NDWI images and using findOverlap() extract areas that overlap with training data. \n",
    "    The overlapping areas in raw images are written in a separate file, and annotation file are created from polygons in the overlapping areas.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(writePath):\n",
    "        os.makedirs(writePath)\n",
    "        \n",
    "    polygonsInAreaDf = gps.GeoDataFrame(areaInfo['polygons'])\n",
    "    img_id=str(areaInfo['id'])\n",
    "    bboxArea = box(*areaInfo['bounds'])\n",
    "\n",
    "    #draw ndwi ong\n",
    "    ndwiImg = rasterio.open(os.path.join(raw_image_base_dir,raw_ndwi_image_prefix+img_id+raw_image_file_type))  \n",
    "    sm_ndwi = rasterio.mask.mask(ndwiImg, [bboxArea], all_touched=True, crop=True )\n",
    "    profile_ndwi = ndwiImg.profile  \n",
    "    profile_ndwi['height'] = sm_ndwi[0].shape[1]\n",
    "    profile_ndwi['width'] = sm_ndwi[0].shape[2]\n",
    "    profile_ndwi['transform'] = sm_ndwi[1]\n",
    "        # That's a problem with rasterio, if the height and the width are less then 256 it throws: ValueError: blockysize exceeds raster height \n",
    "        # So set the blockxsize and blockysize to prevent this problem\n",
    "    profile_ndwi['blockxsize'] = 32\n",
    "    profile_ndwi['blockysize'] = 32\n",
    "    profile_ndwi['count'] = 1\n",
    "    profile_ndwi['dtype'] = rasterio.float32\n",
    "    dt_ndwi = sm_ndwi[0][0].astype(profile_ndwi['dtype'])\n",
    "    with rasterio.open(os.path.join(writePath, extracted_ndwi_filename+'_sup_{}.png'.format(img_id)), 'w', **profile_ndwi) as dst:\n",
    "        dst.write(dt_ndwi, 1) \n",
    "\n",
    "    #draw annotation png\n",
    "    polygons = []\n",
    "    for i in polygonsInAreaDf.index:\n",
    "        gm = polygonsInAreaDf.loc[i]['geometry']\n",
    "        polygons.append(gm)\n",
    "                \n",
    "    with rasterio.open(os.path.join(writePath,extracted_annotation_filename+'_sup_{}.png'.format(img_id)), 'w+', **profile_ndwi) as out:\n",
    "        out_arr = out.read(1)\n",
    "        burned = features.rasterize(polygons, fill=0, default_value=1,out=out_arr, transform=out.transform)\n",
    "        out.write_band(1, burned)\n",
    "        \n",
    "    #draw red green blue png\n",
    "    bandsImg = rasterio.open(os.path.join(raw_image_base_dir,raw_bands_image_prefix+img_id+raw_image_file_type))\n",
    "    sm_bands = rasterio.mask.mask(bandsImg, [bboxArea], all_touched=True, crop=True )\n",
    "    for band, imFn in zip(four_bands, extracted_bands_filename):\n",
    "        dt_bands = sm_bands[0][band].astype(profile_ndwi['dtype'])\n",
    "        with rasterio.open(os.path.join(writePath, imFn+'_sup_{}.png'.format(img_id)), 'w', **profile_ndwi) as dst:\n",
    "            dst.write(dt_bands, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key,value in zip(areasWithPolygons.keys(),areasWithPolygons.values()):\n",
    "    path_to_write=os.path.join(output_dir,'output{}'.format(value['type']))\n",
    "    extractAreasThatOverlapWithTrainingData(value,path_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted image \n",
    "sampleImage = '_55.png'\n",
    "# path_to_write=os.path.join(training_base_dir,'output\\output4')\n",
    "path_to_write=os.path.join(training_base_dir,'output588\\output0' )\n",
    "fn = os.path.join(path_to_write, extracted_NDWI_filename + sampleImage)\n",
    "NDWI_img = Image.open(fn)\n",
    "read_NDWI_img = np.array(NDWI_img)\n",
    "\n",
    "# redBands = os.path.join(path_to_write, 'red'+ sampleImage)\n",
    "# red_img = Image.open(redBands)\n",
    "# read_red_img = np.array(red_img)\n",
    "\n",
    "greenBands = os.path.join(path_to_write, 'green'+ sampleImage)\n",
    "green_img = Image.open(greenBands)\n",
    "read_green_img = np.array(green_img)\n",
    "\n",
    "# blueBands = os.path.join(path_to_write, 'blue'+ sampleImage)\n",
    "# blue_img = Image.open(blueBands)\n",
    "# read_blue_img = np.array(blue_img)\n",
    "\n",
    "swirBands = os.path.join(path_to_write, 'swir'+ sampleImage)\n",
    "swir_img = Image.open(swirBands)\n",
    "read_swir_img = np.array(swir_img)\n",
    "# print(read_NDWI_img.shape)\n",
    "# print(read_Bands_img.shape)\n",
    "annotation_im = Image.open(fn.replace(extracted_ndwi_filename ,config.extracted_annotation_filename))\n",
    "read_annotation = np.array(annotation_im)\n",
    "# print(read_annotation.shape)\n",
    "# print(read_annotation)\n",
    "\n",
    "all_images = np.array([read_NDWI_img,read_green_img,read_swir_img, read_annotation])#,read_red_img,read_blue_img\n",
    "# print(all_images.shape[1])\n",
    "display_images(np.expand_dims(np.transpose(all_images, axes=(1,2,0)), axis=0),['ndwi','green','swir','annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lake_env",
   "language": "python",
   "name": "lake_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'D:\\lakemapping\\U_Net'\n",
    "dataset_dir=r'D:\\lakemapping\\2_dataset\\sample747'\n",
    "# base_dir = r'/media/nkd/backup/5_lakemapping/U_Net'\n",
    "# dataset_dir=r'/media/nkd/backup/5_lakemapping/sample600'\n",
    "image_type = '.png'       \n",
    "NDWI_fn = 'ndwi'\n",
    "red_fn = 'red'\n",
    "blue_fn = 'blue'\n",
    "green_fn = 'green'\n",
    "swir_fn = 'swir'\n",
    "annotation_fn = 'annotation'\n",
    "type_num=6\n",
    "patch_size = (512,512,6) \n",
    "patch_dir = os.path.join(base_dir,'patches{}'.format(patch_size[0])) \n",
    "# The training areas are divided into training, validation and testing set. Note that training area can have different sizes, so it doesn't guarantee that the final generated patches (when using sequential stratergy) will be in the same ratio.\n",
    "test_ratio = 0.2\n",
    "val_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集\n",
    "从output文件夹存储到patchReshape文件夹，train和test按类别type分别存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveFileinType(lakeType,patchType,fn):\n",
    "    output_dir=os.path.join(dataset_dir,r'patchesReshape/{}/type{}'.format(patchType,lakeType))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    move(output_dir,fn)\n",
    "    move(output_dir,fn.replace(NDWI_fn,annotation_fn))\n",
    "    move(output_dir,fn.replace(NDWI_fn,red_fn))\n",
    "    move(output_dir,fn.replace(NDWI_fn,green_fn))\n",
    "    move(output_dir,fn.replace(NDWI_fn,blue_fn))\n",
    "    move(output_dir,fn.replace(NDWI_fn,swir_fn))\n",
    "    \n",
    "def move(output_dir,fn):\n",
    "    new=os.path.join(output_dir,fn)\n",
    "    old=os.path.join(patch_output,fn)\n",
    "    shutil.move(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split dataset and copy images & annotations to new directory\n",
    "for i in range(0,type_num):\n",
    "    frames = []\n",
    "    frames_json = os.path.join(patch_dir,'frames_list{}.json'.format(i))\n",
    "    patch_output = os.path.join(dataset_dir,'output/output{}'.format(i))\n",
    "    all_files = os.listdir(patch_output)\n",
    "    all_files_NDWI = [fn for fn in all_files if fn.startswith(NDWI_fn) and fn.endswith(image_type)]#ndwi.png\n",
    "    len(all_files_NDWI)\n",
    "    print(all_files_NDWI)\n",
    "\n",
    "    if os.path.isfile(frames_json):\n",
    "        print(\"dataset type{} had been splited\".format(i))\n",
    "#             train data are classified into different dirs for following process.\n",
    "        with open(frames_json, 'r') as file:\n",
    "            fjson = json.load(file)#\n",
    "            for train_fn in fjson['training_frames']:\n",
    "                moveFileinType(i,'train',train_fn)\n",
    "            for testing_fn in fjson['testing_frames']:\n",
    "                moveFileinType(i,'test',testing_fn)\n",
    "            for validation_fn in fjson['validation_frames']:\n",
    "                moveFileinType(i,'val',validation_fn)\n",
    "    else:\n",
    "            print(\"Creating and writing train-test split from file\")\n",
    "            frames_list = list(range(len(all_files_NDWI)))\n",
    "            \n",
    "            # Divide into training and test set       \n",
    "            training_frames, testing_frames = train_test_split(frames_list, test_size=test_ratio)\n",
    "            # Further divide into training set into training and validataion set              \n",
    "            training_frames, validation_frames = train_test_split(training_frames, test_size=val_ratio)\n",
    "            \n",
    "            training_frames_name=[all_files_NDWI[id] for id in training_frames]\n",
    "            testing_frames_name=[all_files_NDWI[id] for id in testing_frames]\n",
    "            validation_frames_name=[all_files_NDWI[id] for id in validation_frames]\n",
    "            \n",
    "            #train data are classified into different dirs for following process.\n",
    "#             for train_fn in training_frames_name:\n",
    "#                 moveFileinType(i,'train',train_fn)\n",
    "#             for testing_fn in testing_frames_name:\n",
    "#                 moveFileinType(i,'test',testing_fn) \n",
    "#             for validation_fn in validation_frames_name:\n",
    "#                 moveFileinType(i,'val',validation_fn)\n",
    "                \n",
    "            frame_split = {\n",
    "                'training_frames': training_frames_name,\n",
    "                'testing_frames': testing_frames_name,\n",
    "                'validation_frames': validation_frames_name\n",
    "            }\n",
    "            \n",
    "            if not os.path.exists(patch_dir):\n",
    "                os.makedirs(patch_dir)\n",
    "            with open(frames_json, 'w') as f:\n",
    "                json.dump(frame_split, f)\n",
    "                \n",
    "            print('training_frames', training_frames_name)\n",
    "            print('validation_frames',validation_frames_name )\n",
    "            print('testing_frames', testing_frames_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lake_env",
   "language": "python",
   "name": "lake_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

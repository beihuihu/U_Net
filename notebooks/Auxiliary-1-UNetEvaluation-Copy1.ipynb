{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Author: Ankit Kariryaa, University of Bremen\n",
    "  \n",
    "  Modified by Xuehui Pi and Qiuqi Luo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-rc3\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np               # numerical array manipulation\n",
    "import pandas as pd\n",
    "import geopandas as gps\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry import mapping, shape\n",
    "import fiona\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import rasterio.mask\n",
    "import affine\n",
    "\n",
    "from core.UNet import UNet\n",
    "from core.losses import tversky, accuracy, dice_coef, dice_loss, IoU, recall, precision\n",
    "from core.optimizers import adaDelta, adagrad, adam, nadam\n",
    "from core.frame_info import FrameInfo\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.visualize import display_images\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto(\n",
    "    #device_count={\"CPU\": 64},\n",
    "    allow_soft_placement=True, \n",
    "    log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the data related variables used in the notebook \n",
    "\n",
    "# For reading the GSW and annotated images generated in the step - 1\n",
    "\n",
    "base_dir = r'D:\\sample250\\U-Net_13' \n",
    "type_num = 5\n",
    "image_type = '.png'\n",
    "NDWI_fn = 'ndwi'\n",
    "red_fn = 'red'\n",
    "blue_fn = 'blue'\n",
    "green_fn = 'green'\n",
    "swir_fn = 'swir'\n",
    "annotation_fn = 'annotation'\n",
    "\n",
    "# For testing, images are divided into sequential patches \n",
    "patch_generation_stratergy = 'sequential'\n",
    "patch_size = (512,512,2) ## Height * Width * (Input or Output) channelsï¼š[GSW, ANNOTATION]\n",
    "BATCH_SIZE = 16 # Model is evaluated in batches; See https://keras.io/models/model/\n",
    "\n",
    "# # When stratergy == sequential\n",
    "step_size = (512,512)\n",
    "\n",
    "# input_shape = (512,512,5)\n",
    "# input_image_channel = [0,1,2,3,4]\n",
    "# input_label_channel = [5]\n",
    "\n",
    "# input_shape = (512,512,2)\n",
    "# input_image_channel = [0,1]\n",
    "# input_label_channel = [2]\n",
    "\n",
    "input_shape = (512,512,1)\n",
    "input_image_channel = [0]\n",
    "input_label_channel = [1]\n",
    "\n",
    "OPTIMIZER_NAME = 'adaDelta'\n",
    "OPTIMIZER = adaDelta \n",
    "# OPTIMIZER=tf.train.experimental.enable_mixed_precision_graph_rewrite(OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = dice_loss\n",
    "# LOSS=tf.keras.losses.BinaryCrossentropy()\n",
    "# LOSS_NAME = 'tversky'\n",
    "LOSS_NAME = 'dice_loss'\n",
    "# modelToEvaluate = os.path.join(base_dir, r'saved_models\\UNet\\lakes_20230818-2031_AdaDelta_dice_loss_b5_012345_512.h5')\n",
    "# modelToEvaluate = os.path.join(base_dir, r'saved_models\\UNet\\lakes_20230819-0235_AdaDelta_dice_loss_b5_normalized_012345_512.h5')\n",
    "# modelToEvaluate = os.path.join(base_dir, r'saved_models\\UNet\\lakes_20230819-0232_AdaDelta_dice_loss_b2_normalized_012_512.h5')\n",
    "modelToEvaluate = os.path.join(base_dir, r'saved_models\\UNet\\lakes_20230819-1052_AdaDelta_dice_loss_b1_01_512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\sample250\\U-Net_13\\evaluationreport\\evaluation_per_pixel20230819-1507_012.csv\n"
     ]
    }
   ],
   "source": [
    "#File path for final report \n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = input_image_channel + input_label_channel\n",
    "chs = reduce(lambda a,b: a+str(b),   chf, '')\n",
    "evaluation_report_path = model_path =  os.path.join(base_dir, 'evaluationreport') \n",
    "if not os.path.exists(evaluation_report_path):\n",
    "    os.makedirs(evaluation_report_path)\n",
    "evaluation_report_filename = os.path.join(evaluation_report_path,'evaluation_per_pixel{}_{}.csv'.format(timestr,chs))\n",
    "print(evaluation_report_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBands(path_to_write,fn):\n",
    "    img=rasterio.open(os.path.join(path_to_write,fn))\n",
    "    \n",
    "#     read_img=img.read()/1000\n",
    "    \n",
    "    im=img.read()\n",
    "    axis=(0, 1)\n",
    "    read_img=(im - im.mean(axis)) / (im.std(axis) + 1e-8)\n",
    "    return read_img\n",
    "\n",
    "def readImgs(path_to_write, fn):\n",
    "    NDWI_img = rasterio.open(os.path.join(path_to_write, fn))\n",
    "    read_NDWI_img = NDWI_img.read()/100\n",
    "    \n",
    "    comb_img = np.transpose(read_NDWI_img, axes=(1,2,0))\n",
    "    \n",
    "    #if use other bands for trainging, use codes as belows:\n",
    "#     read_red_img =readBands(path_to_write,fn.replace(NDWI_fn ,red_fn))\n",
    "#     read_green_img =readBands(path_to_write,fn.replace(NDWI_fn ,green_fn))\n",
    "#     read_blue_img = readBands(path_to_write,fn.replace(NDWI_fn ,blue_fn))\n",
    "#     read_swir_img = readBands(path_to_write, fn.replace(NDWI_fn ,swir_fn))\n",
    "#     comb_img = np.concatenate((read_NDWI_img,read_swir_img), axis=0)\n",
    "#     comb_img = np.concatenate((read_NDWI_img,read_red_img,read_green_img,read_blue_img, read_swir_img), axis=0)\n",
    "#     comb_img = np.transpose(comb_img, axes=(1,2,0)) #Channel at the end  ( , ,1) \n",
    "    \n",
    "    annotation_im = Image.open(os.path.join(path_to_write, fn.replace(NDWI_fn,annotation_fn)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    \n",
    "    f = FrameInfo(comb_img, annotation)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type0 image number:14\n",
      "type1 image number:14\n",
      "type2 image number:5\n",
      "type3 image number:11\n",
      "type4 image number:1\n",
      "total img count:45\n",
      "test patchs number: 773\n"
     ]
    }
   ],
   "source": [
    "#load testing dataset, use all sequential patchs\n",
    "frames=[]\n",
    "for i in range(0,type_num):\n",
    "    path_to_write=os.path.join(base_dir,'patchesReshape\\\\test\\\\type'+str(i))\n",
    "    all_files = os.listdir(path_to_write)\n",
    "    all_files_NDWI = [fn for fn in all_files if fn.startswith(NDWI_fn) and fn.endswith(image_type)]#ndwi.png\n",
    "    print('type{} image number:{}'.format(i,len(all_files_NDWI)))\n",
    "    for j, fn in enumerate(all_files_NDWI):\n",
    "        f =readImgs(path_to_write,fn)\n",
    "        frames.append(f)\n",
    "random.shuffle(frames)\n",
    "print('total img count:'+str(len(frames)))\n",
    "test_patchs = DataGenerator(input_image_channel, patch_size, frames, input_label_channel, augmenter = None).all_sequential_patches(step_size)\n",
    "print('test patchs number:',len(test_patchs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\sample250\\U-Net_13\\saved_models\\UNet\\lakes_20230819-1052_AdaDelta_dice_loss_b1_01_512.h5 D:\\sample250\\U-Net_13\\evaluationreport\\evaluation_per_pixel20230819-1507_012.csv\n",
      "Evaluating model now!\n",
      "25/25 [==============================] - 16s 581ms/step - loss: 0.2739 - dice_loss: 0.2752 - accuracy: 0.9919 - recall: 0.7265 - precision: 0.7726 - IoU: 0.5877\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model \n",
    "\n",
    "def evaluate_model(model_path, evaluation_report_filename):\n",
    "    print(model_path, evaluation_report_filename)\n",
    "    model = load_model(model_path, custom_objects={'dice_loss':dice_loss, 'accuracy':accuracy , 'recall':recall, 'precision':precision,'IoU':IoU}, compile=False)\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_loss, accuracy,recall,precision,IoU])\n",
    "    \n",
    "    print('Evaluating model now!')\n",
    "    ev = model.evaluate(test_patchs[0], test_patchs[1],  verbose=1, use_multiprocessing=False)\n",
    "    report  = dict(zip(model.metrics_names, ev))\n",
    "    report['model_path'] =  model_path   \n",
    "    report['test_frame_dir']= base_dir   \n",
    "    report['total_patch_count']= len(test_patchs[0])  \n",
    "    return report\n",
    "\n",
    "report = evaluate_model(modelToEvaluate, evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27391839027404785, 'dice_loss': 0.2752492129802704, 'accuracy': 0.9919134378433228, 'recall': 0.7264832258224487, 'precision': 0.7725878953933716, 'IoU': 0.5876513123512268, 'model_path': 'D:\\\\sample250\\\\U-Net_13\\\\saved_models\\\\UNet\\\\lakes_20230819-1052_AdaDelta_dice_loss_b1_01_512.h5', 'test_frame_dir': 'D:\\\\sample250\\\\U-Net_13', 'total_patch_count': 773}\n",
      "Index(['loss', 'dice_loss', 'accuracy', 'recall', 'precision', 'IoU',\n",
      "       'model_path', 'test_frame_dir', 'total_patch_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Generate the final report\n",
    "print(report)\n",
    "\n",
    "tdf = pd.DataFrame(report, index=[0])  \n",
    "print(tdf.columns)\n",
    "col_beginning = ['model_path','test_frame_dir', 'total_patch_count', 'accuracy', 'recall','precision','IoU']\n",
    "\n",
    "col_rest = [x for x in tdf.columns.tolist() if x not in col_beginning]\n",
    "cols = col_beginning + col_rest\n",
    "tdf = tdf[cols]\n",
    "tdf.to_csv(evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
